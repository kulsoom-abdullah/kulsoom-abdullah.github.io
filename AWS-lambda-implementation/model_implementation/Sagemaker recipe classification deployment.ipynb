{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe cuisine classification - training and deployment\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "* [Description](#description)\n",
    "* [Getting the data from S3](#get_data)\n",
    "* [Text Processing](#process)\n",
    " * [Writing a Custom Transformer](#custom_tx)\n",
    " * [Words to remove](#words_remove)\n",
    "* [Create SKLearn pipeline](#pipeline)\n",
    "* [Deploy the model](#deploy)\n",
    "* [Test the prediction function](#prediction_request)\n",
    "* [Endpoint cleanup](#endpoint_cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description <a class=\"anchor\" id=\"description\"></a>\n",
    "This Sagemaker notebook trains and fits a logistic regression model on recipe data that has been scraped using lambda functions.  More detail on this project is in my Github repo and blog post. This model is pickeled and served as an endpoint. Prediction examples are shown inline in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data from S3 <a class=\"anchor\" id=\"get_data\"></a>\n",
    "The scraped recipe data is in an S3 bucket, *recipes-allrecipes*.  Each recipe is stored as a json file.  The code below retrieves these json files, reads each into a pandas dataframe, and formats the ingredients into a list.  That is, each row will be one recipe, and the ingredients column will contain a list format of all of the ingredients.  To get the columns in the same level, I use the Dataframe function `.to_records()` line shown here:\n",
    "\n",
    "`\n",
    "test_df=pd.DataFrame(recipe_df.to_records())\n",
    "`\n",
    "\n",
    "to flatten the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('recipes-allrecipes')\n",
    "\n",
    "bucket_recipes = \"recipes-allrecipes\"\n",
    "recipe_df=None \n",
    "try:\n",
    "#     if len(objs) > 0:\n",
    "    for object in my_bucket.objects.all():\n",
    "        key=object.key\n",
    "        if (key[-5:]=='.json'): #check that this is a .json file\n",
    "            data_location = 's3://{}/{}'.format(bucket_recipes, key)\n",
    "            df=pd.read_json(data_location)\n",
    "            df=df.groupby(['id', 'title', 'description', 'rating', 'reviews','url','category'])['ingredients'].apply(list).to_frame()\n",
    "\n",
    "        if(recipe_df is None):\n",
    "            recipe_df=df\n",
    "        else:\n",
    "            recipe_df = pd.concat([recipe_df,df])\n",
    "\n",
    "except KeyError:\n",
    "    print(\"No objects in the s3 bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the dataframe\n",
    "test_df=pd.DataFrame(recipe_df.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             3653\n",
       "title          3653\n",
       "description    3653\n",
       "rating         3653\n",
       "reviews        3653\n",
       "url            3653\n",
       "category       3653\n",
       "ingredients    3653\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing <a class=\"anchor\" id=\"process\"></a>\n",
    "Now that the data is retrieved, I need to process the data for the model to train on.  I use the work from my local Jupyter notebook and make that work here in Sagemaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a Custom Transformer <a class=\"anchor\" id=\"custom_tx\"></a>\n",
    "When working locally on the data, I did not perform any predictions as I was using the data I had scraped, divided it into train/test to determine the model performance.  Since I will be using the SKLearn pipeline for implementation and I plan to use it for predicting on new data (in the API endpoint), I need to format the `parse_recipes` function to use in a scikit-learn pipeline.  It is not necessary but I would rather have it all in one pipeline.\n",
    "\n",
    "FunctionTransformer (simpler functions), Fixing for state\n",
    "\n",
    "A `FunctionTransformer` class helps to introduce arbitrary, stateless transforms into a `Pipeline`.  Using a `TransformerMixin` class is overkill for what I need to do, though since I was able to get it to work with the recipe data, I am using it for now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class TransformRecipe(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self,X, **kwargs):\n",
    "        recipe_list=X\n",
    "        ingr_list=[]\n",
    "        bigram_list=[]\n",
    "        recipe_string_list=[]\n",
    "\n",
    "#         ilist = [[word.lower() for word in i.split()] for i in ilist] \n",
    "        for recipe in recipe_list:\n",
    "#             print(recipe)\n",
    "            ingr_list=[]\n",
    "            for ingredient in recipe:\n",
    "\n",
    "                ingredient.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "                words = ingredient.split()\n",
    "                words = [''.join(c for c in word if c not in string.punctuation) for word in words]\n",
    "                words = [word for word in words if word.isalpha()]\n",
    "                words = [word.lower() for word in words] \n",
    "                words = [lemmatizer.lemmatize(word) for word in words]\n",
    "                words = [word for word in words if word not in measures]\n",
    "                words = [word for word in words if word not in common_remove]\n",
    "                words = [word for word in words if word not in data_leaks]\n",
    "                #get rid of any blank\n",
    "                words = list(filter(None, words))\n",
    "#                 print(\"before if length statements\")\n",
    "#                 print(words)\n",
    "                if(len(words)<=3):\n",
    "                    ingr_list.append(' '.join(words))\n",
    "\n",
    "                words = [word for word in words if word not in useless_singles]\n",
    "            #easiest way to deal with any duplicates or blanks for now\n",
    "                ingr_list = list(set(ingr_list))\n",
    "                #attempts to get rid of the blank\n",
    "#                 ingr_list = list(filter(None, ingr_list))\n",
    "#                 ingr_list = [x for x in ingr_list if x]   \n",
    "                if(len(words)>3): #handle rare case\n",
    "                    ingr_list.append(' '.join(words))\n",
    "                recipe_string=' '.join(ingr_list)\n",
    "#             print(recipe_string)\n",
    "\n",
    "            recipe_string_list.append(recipe_string)\n",
    "\n",
    "        return recipe_string_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words to Remove <a class=\"anchor\" id=\"words_remove\"></a>\n",
    "This code and these word lists are copied over as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Source for list below\n",
    "#https://en.wikipedia.org/wiki/Cooking_weights_and_measures\n",
    "#https://thebakingpan.com/ingredient-weights-and-measures/\n",
    "measures=['litrbes','liter','millilitres','mL','grams','g', 'kg','teaspoon','tsp', 'tablespoon','tbsp','fluid', 'ounce','oz','fl.oz', 'cup','pint','pt','quart','qt','gallon','gal','smidgen','drop','pinch','dash','scruple','dessertspoon','teacup','cup','c','pottle','gill','dram','wineglass','coffeespoon','pound','pounded','lb','tbsp','plus','firmly', 'packed','lightly','level','even','rounded','heaping','heaped','sifted','bushel','peck','stick','chopped','sliced','halves', 'shredded','slivered','sliced','whole','paste','whole',' fresh', 'peeled', 'diced','mashed','dried','frozen','fresh','peeled','candied','no', 'pulp','crystallized','canned','crushed','minced','julienned','clove','head', 'small','large','medium', 'torn', 'cleaned', 'degree']\n",
    "\n",
    "measures = [lemmatizer.lemmatize(m) for m in measures]\n",
    "#some of these include data leakage words, like 'italian' - ok to remove after including bigrams\n",
    "data_leaks = ['italianstyle', 'french','thai', 'chinese', 'mexican','spanish','indian','italian']\n",
    "\n",
    "common_remove=['ground','to','taste', 'and', 'or',  'can',  'into', 'cut', 'grated', 'leaf','package','finely','divided','a','piece','optional','inch','needed','more','drained','for','flake','dry','thinly','cubed','bunch','cube','slice','pod','beaten','seeded','uncooked','root','plain','heavy','halved','crumbled','sweet','with','hot','room','temperature','trimmed','allpurpose','deveined','bulk','seasoning','jar','food','if','bag','mix','in','each','roll','instant','double','such','frying','thawed','whipping','stock','rinsed','mild','sprig','freshly','toasted','link','boiling','cooked','unsalted','container',\n",
    "'cooking','thin','lengthwise','warm','softened','thick','quartered','juiced','pitted','chunk','melted','cold','coloring','puree','cored','stewed','floret','coarsely','the','blanched','zested','sweetened','powdered','garnish','dressing','soup','at','active','lean','chip','sour','long','ripe','skinned','fillet','from','stem','flaked','removed','stalk','unsweetened','cover','crust', 'extra', 'prepared', 'blend', 'of', 'ring',  'undrained', 'about', 'zest', ' ', '', 'spray', 'round', 'herb', 'seasoned', 'wedge', 'bitesize', 'broken', 'square', 'freshly', 'thickly', 'diagonally']\n",
    "common_remove = [lemmatizer.lemmatize(c) for c in common_remove]\n",
    "data_leaks = [lemmatizer.lemmatize(d) for d in data_leaks]\n",
    "# due to using bigrams not including \n",
    "useless_singles=['','black','white','red','yellow','seed','breast','confectioner','sundried','broth','bell','baby','juice','crumb','sauce','condensed','smoked','basmati','extravirgin','brown','clarified', 'soy', 'filling', 'pine', 'virgin', 'romano', 'heart', 'shell', 'thigh', 'boneless','skinless','split', 'dark', 'wheat', 'light', 'green', 'vegetable', 'curry', 'orange', 'garam', 'sesame', 'strip', 'sea', 'canola', 'mustard','powder', 'ice', 'bay', 'roasted', 'loaf', 'roast', 'powder']\n",
    "useless_singles = [lemmatizer.lemmatize(u) for u in useless_singles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SKLearn Pipeline <a class=\"anchor\" id=\"pipeline\"></a>\n",
    "Here I put together the pipeline, using the class weighted Logistic Regression model.  Additionally, the TransformRecipe class I wrote above is used here as part of the pipeline. It was easier for me to work with the data as a list of lists instead of a pandas dataframe, I can use a list or list of lists as input to the API, and since it works, I won't change it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "results for LR count vector\n",
      "{   'fit_time': array([2.373698  , 2.86831737, 2.66116476]),\n",
      "    'score_time': array([0.63286567, 0.77275896, 0.68454027]),\n",
      "    'test_score': array([0.85739242, 0.85650425, 0.85730251]),\n",
      "    'train_score': array([0.99426657, 0.99266337, 0.99468617])}\n",
      "Done writing to s3n://lrpickle/model.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate,StratifiedKFold, train_test_split\n",
    "import pprint\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3)\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "print(\"\\nresults for LR count vector\")\n",
    "lr_pipe = Pipeline([('parse_recipe_text', TransformRecipe()),\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 2))), \n",
    "    ('lr', LogisticRegression( max_iter=1000,random_state=123, \n",
    "    class_weight='balanced',multi_class='multinomial', solver='lbfgs'))])\n",
    "\n",
    "\n",
    "cv=cross_validate(lr_pipe, test_df['ingredients'], test_df['category'].values, scoring='f1_weighted', \n",
    "                         cv=skf, return_train_score=True )\n",
    "\n",
    "pp.pprint(cv)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_df['ingredients'].values, test_df['category'], test_size=0.25, stratify=test_df['category'].tolist())\n",
    "\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "# #Pickle pipeline after caling the fit\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lr_pipe, 'lr_pipe.pkl')\n",
    "\n",
    "# This puts the pickled model into an S3 bucket\n",
    "key = 'model.pkl'\n",
    "bucket='lrpickle'\n",
    "url = 's3n://{}/{}'.format(bucket, key)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_file('lr_pipe.pkl')\n",
    "print('Done writing to {}'.format(url))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Deploy the model <a class=\"anchor\" id=\"deploy\"></a>\n",
    "\n",
    "If you want to serve the model using Sagemaker, below is commented code that was in this sagemaker template notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.deploy\n",
    "# predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the prediction function <a class=\"anchor\" id=\"prediction_request\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['indian', 'french'], dtype='<U7')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_list = [\n",
    "    ['1 egg, lightly beaten', '1 pound ground beef', '1 tomato, finely chopped',\n",
    "              '1 red onion, finely chopped','1/4 cup finely chopped cilantro','1/4 cup finely chopped mint',\n",
    "             '2 teaspoons ginger-garlic paste','2 teaspoons coriander seeds, crushed','1 teaspoon salt',\n",
    "             '3/4 teaspoon ground cumin','3/4 teaspoon ground cayenne pepper',\n",
    "              '1/4 cup vegetable oil for frying, or more as needed', '2 tomatoes, sliced into rounds'],\n",
    "    ['vanilla ice cream', 'shortcake', 'sliced strawberries', 'whipped cream']\n",
    "    ]\n",
    "\n",
    "\n",
    "lr_pipe.predict(test_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad ... The first recipe is actually a northern Pakistani one, but of the 6 classes, Indian would be the best match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint cleanup <a class=\"anchor\" id=\"endpoint_cleanup\"></a>\n",
    "\n",
    "When you're done with the endpoint, you'll want to clean it up if using Sagemaker to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the parse recipe function, I did not need to use a TransformerMixin. I keep it below for my future reference.\n",
    "# import string\n",
    "# from sklearn.base import TransformerMixin\n",
    "\n",
    "# class TransformRecipe(TransformerMixin):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "#     def fit(self, X, y):\n",
    "#         return self\n",
    "#     def transform(self,X, **kwargs):\n",
    "#         recipe_list=X\n",
    "#         ingr_list=[]\n",
    "#         bigram_list=[]\n",
    "#         recipe_string_list=[]\n",
    "#         for recipe in recipe_list:\n",
    "# #             print(recipe)\n",
    "#             ingr_list=[]\n",
    "#             for ingredient in recipe:\n",
    "#                 ingredient.translate(str.maketrans('', '', string.punctuation))\n",
    "#                 words = ingredient.split()\n",
    "#                 words = [''.join(c for c in word if c not in string.punctuation) for word in words]\n",
    "#                 words = [word for word in words if word.isalpha()]\n",
    "#                 words = [word.lower() for word in words] \n",
    "#                 words = [lemmatizer.lemmatize(word) for word in words]\n",
    "#                 words = [word for word in words if word not in measures]\n",
    "#                 words = [word for word in words if word not in common_remove]\n",
    "#                 words = [word for word in words if word not in data_leaks]\n",
    "#                 #get rid of any blanks\n",
    "#                 words = list(filter(None, words))\n",
    "#                 if(len(words)<=3):\n",
    "#                     ingr_list.append(' '.join(words))\n",
    "\n",
    "#                 words = [word for word in words if word not in useless_singles]\n",
    "\n",
    "#             #easiest way to deal with any duplicates or blanks for now\n",
    "#                 ingr_list = list(set(ingr_list)) \n",
    "#                 if(len(words)>3): #handle rare case\n",
    "#                     ingr_list.append(' '.join(words))\n",
    "#                 recipe_string=' '.join(ingr_list)\n",
    "\n",
    "#             recipe_string_list.append(recipe_string)\n",
    "\n",
    "#         return recipe_string_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
